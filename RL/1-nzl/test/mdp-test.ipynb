{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    '''A simple MDP class.  It includes the following members'''\n",
    "\n",
    "    def __init__(self,T,R,discount):\n",
    "        '''Constructor for the MDP class\n",
    "\n",
    "        Inputs:\n",
    "        T -- Transition function: |A| x |S| x |S'| array\n",
    "        R -- Reward function: |A| x |S| array\n",
    "        discount -- discount factor: scalar in [0,1)\n",
    "\n",
    "        The constructor verifies that the inputs are valid and sets\n",
    "        corresponding variables in a MDP object'''\n",
    "\n",
    "        assert T.ndim == 3, \"Invalid transition function: it should have 3 dimensions\"\n",
    "        self.nActions = T.shape[0]\n",
    "        self.nStates = T.shape[1]\n",
    "        assert T.shape == (self.nActions,self.nStates,self.nStates), \"Invalid transition function: it has dimensionality \" + repr(T.shape) + \", but it should be (nActions,nStates,nStates)\"\n",
    "        assert (abs(T.sum(2)-1) < 1e-5).all(), \"Invalid transition function: some transition probability does not equal 1\"\n",
    "        self.T = T\n",
    "        assert R.ndim == 2, \"Invalid reward function: it should have 2 dimensions\" \n",
    "        assert R.shape == (self.nActions,self.nStates), \"Invalid reward function: it has dimensionality \" + repr(R.shape) + \", but it should be (nActions,nStates)\"\n",
    "        self.R = R\n",
    "        assert 0 <= discount < 1, \"Invalid discount factor: it should be in [0,1)\"\n",
    "        self.discount = discount\n",
    "        \n",
    "    def valueIteration(self, initialV, nIterations=np.inf, tolerance=0.01):\n",
    "        '''Value iteration procedure\n",
    "        V <-- max_a R^a + gamma T^a V\n",
    "\n",
    "        Inputs:\n",
    "        initialV -- Initial value function: array of |S| entries\n",
    "        nIterations -- limit on the # of iterations: scalar (default: infinity)\n",
    "        tolerance -- threshold on ||V^n-V^n+1||_inf: scalar (default: 0.01)\n",
    "\n",
    "        Outputs: \n",
    "        V -- Value function: array of |S| entries\n",
    "        iterId -- # of iterations performed: scalar\n",
    "        epsilon -- ||V^n-V^n+1||_inf: scalar'''\n",
    "        \n",
    "        # temporary values to ensure that the code compiles until this\n",
    "        # function is coded\n",
    "        V = initialV.copy()\n",
    "        iterId = 0\n",
    "        epsilon = tolerance\n",
    "        while iterId<nIterations and epsilon>=tolerance:\n",
    "            V_old = V.copy() \n",
    "            Q_sa = np.zeros((self.nActions, self.nStates)) \n",
    "            for a in range(self.nActions):\n",
    "                Q_sa[a] = self.R[a] + self.discount*np.matmul(self.T[a], V_old)\n",
    "\n",
    "            for s in range(self.nStates):\n",
    "                V[s] = np.max(Q_sa[:,s])\n",
    "\n",
    "            iterId += 1\n",
    "            epsilon = np.max(np.abs(V - V_old))\n",
    "\n",
    "        return [V, iterId, epsilon]\n",
    "\n",
    "    def extractPolicy(self, V):\n",
    "        '''Procedure to extract a policy from a value function\n",
    "        pi <-- argmax_a R^a + gamma T^a V\n",
    "\n",
    "        Inputs:\n",
    "        V -- Value function: array of |S| entries\n",
    "\n",
    "        Output:\n",
    "        policy -- Policy: array of |S| entries'''\n",
    "\n",
    "        # temporary values to ensure that the code compiles until this\n",
    "        # function is coded\n",
    "        policy = np.zeros(self.nStates)\n",
    "        Q_sa = np.zeros((self.nActions, self.nStates)) \n",
    "        for a in range(self.nActions):\n",
    "            Q_sa[a] = self.R[a] + self.discount*np.matmul(self.T[a], V)\n",
    "\n",
    "        for s in range(self.nStates):\n",
    "            policy[s] = np.argmax(Q_sa[:,s])\n",
    "\n",
    "        return policy \n",
    "\n",
    "    def evaluatePolicy(self, policy):\n",
    "        '''Evaluate a policy by solving a system of linear equations\n",
    "        V^pi = R^pi + gamma T^pi V^pi\n",
    "\n",
    "        Input:\n",
    "        policy -- Policy: array of |S| entries\n",
    "\n",
    "        Ouput:\n",
    "        V -- Value function: array of |S| entries'''\n",
    "\n",
    "        # temporary values to ensure that the code compiles until this\n",
    "        # function is coded\n",
    "        from numpy.linalg import inv\n",
    "\n",
    "        V = np.zeros(self.nStates)\n",
    "        R = np.zeros(self.nStates)\n",
    "        T = np.zeros((self.nStates, self.nStates))\n",
    "\n",
    "        for i in range(self.nStates):\n",
    "            R[i] = self.R[int(policy[i]), i]\n",
    "            T[i] = self.T[int(policy[i]), i]\n",
    "\n",
    "        I = np.identity(self.nStates)\n",
    "        V = np.matmul(inv(I - self.discount * T), R)\n",
    "        #V = np.linalg.solve(I- self.discount * T, R)\n",
    "\n",
    "        return V\n",
    "        \n",
    "        \n",
    "    def policyIteration(self, initialPolicy, nIterations=np.inf):\n",
    "        '''Policy iteration procedure: alternate between policy\n",
    "        evaluation (solve V^pi = R^pi + gamma T^pi V^pi) and policy\n",
    "        improvement (pi <-- argmax_a R^a + gamma T^a V^pi).\n",
    "\n",
    "        Inputs:\n",
    "        initialPolicy -- Initial policy: array of |S| entries\n",
    "        nIterations -- limit on # of iterations: scalar (default: inf)\n",
    "\n",
    "        Outputs: \n",
    "        policy -- Policy: array of |S| entries\n",
    "        V -- Value function: array of |S| entries\n",
    "        iterId -- # of iterations peformed by modified policy iteration: scalar'''\n",
    "\n",
    "        # temporary values to ensure that the code compiles until this\n",
    "        # function is coded\n",
    "        policy = initialPolicy.copy()\n",
    "        iterId = 0\n",
    "\n",
    "        while iterId < nIterations:\n",
    "            policy_old = policy.copy()\n",
    "            V = self.evaluatePolicy(policy_old)\n",
    "            policy = self.extractPolicy(V)\n",
    "            if np.array_equal(policy, policy_old):\n",
    "                break\n",
    "            iterId += 1\n",
    "\n",
    "        return [policy, V, iterId]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.array([[[0.5,0.5,0,0],[0,1,0,0],[0.5,0.5,0,0],[0,1,0,0]], \\\n",
    "\t\t\t  [[1,0,0,0],[0.5,0,0,0.5],[0.5,0,0.5,0],[0,0,0.5,0.5]]])\n",
    "\t\t\t  \n",
    "# Reward function: |A| x |S| array\n",
    "R = np.array([[0,0,10,10], \\\n",
    "\t\t\t  [0,0,10,10]])\n",
    "\t\t\t  \n",
    "# Discount factor: scalar in [0,1)\n",
    "discount = 0.9        \n",
    "\n",
    "# MDP object\n",
    "mdp = MDP(T,R,discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.49636306 38.51527513 43.935435   54.1128575 ] 58 0.009860138819838937\n",
      "[0. 1. 1. 1.]\n",
      "[1.05415115e-15 0.00000000e+00 1.81818182e+01 1.00000000e+01]\n"
     ]
    }
   ],
   "source": [
    "[V,nIterations,epsilon] = mdp.valueIteration(initialV=np.zeros(mdp.nStates))\n",
    "print(V, nIterations, epsilon)\n",
    "\n",
    "policy = mdp.extractPolicy(V)\n",
    "print(policy)\n",
    "\n",
    "V = mdp.evaluatePolicy(np.array([1, 0, 1, 0]))\n",
    "print (V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. 1.] [31.58510431 38.60401638 44.02417625 54.20159875] 1\n"
     ]
    }
   ],
   "source": [
    "[policy,V,iterId] = mdp.policyIteration(np.array([0,0,0,0]))\n",
    "print(policy,V,iterId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
