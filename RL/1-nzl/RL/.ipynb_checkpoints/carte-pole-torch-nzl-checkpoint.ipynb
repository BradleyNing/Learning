{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.seed(1); torch.manual_seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(torch.nn.Module):\n",
    "    def __init__(self, n_inputs=4, n_hidden=10, n_outputs=2, learning_rate=0.01, gamma=0.99):\n",
    "        super(Policy, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(n_inputs, n_hidden) \n",
    "        self.layer2 = torch.nn.Linear(n_hidden, n_outputs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.episode_data = []\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=1e-2)\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "        self.loss = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #l1z = self.layer1(x)\n",
    "        l1a = F.relu(self.layer1(x))\n",
    "        \n",
    "        l2z = self.layer2(l1a)\n",
    "        out = F.softmax(l2z, dim=0)\n",
    "        return out    \n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        pr = self.forward(state)\n",
    "        pr = pr.detach().numpy()\n",
    "        action = np.random.choice(len(pr), p=pr)\n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        R = 0\n",
    "        rewards = []\n",
    "        probs = np.array(self.episode_data)[:,0]\n",
    "        ep_rewards = np.array(self.episode_data)[:,1]\n",
    "        for r in reversed(ep_rewards):\n",
    "            R = r + self.gamma*R\n",
    "            rewards.insert(0, R)\n",
    "        rewards = torch.Tensor(rewards)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + self.eps)\n",
    "            \n",
    "        self.loss = np.dot(-probs, rewards)\n",
    "        #loss = Variable(torch.Tensor([loss]))\n",
    "        #self.zero_grad()\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        self.optimizer.step()\n",
    "#         with torch.no_grad():\n",
    "#             for param in self.parameters():\n",
    "#                 param.data -= self.learning_rate*param.grad\n",
    "#         print(self.episode_data) # debug\n",
    "#         print(probs)\n",
    "#         print(ep_rewards)\n",
    "#         print(rewards)\n",
    "        #self.loss = loss\n",
    "        self.episode_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n",
      "Episode 0\tLast length:    13\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.seed(1); torch.manual_seed(1);\n",
    "episode_num=1; render_num=200; log_interval=100;\n",
    "policy = Policy()\n",
    "for ep_idx in range(episode_num):\n",
    "    state = env.reset()\n",
    "    state = Variable(torch.Tensor(state))\n",
    "    for step_idx in range(1000):\n",
    "        action_probs = policy.forward(state)\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "#             action_probs = action_probs.detach().numpy()\n",
    "#             action = np.random.choice(len(action_probs), p=action_probs)\n",
    "#             action_prob = action_probs[action]\n",
    "        next_s, reward, done, _ = env.step(action.item())\n",
    "        if ep_idx > render_num:\n",
    "            env.render()\n",
    "        policy.episode_data.append((m.log_prob(action), reward))\n",
    "        if done:\n",
    "            break\n",
    "        state = Variable(torch.Tensor(next_s))\n",
    "\n",
    "    policy.learn()\n",
    "    print('\\rEpisode {}\\tLast length: {:5d}'.format(ep_idx, step_idx+1), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.distributions import Categorical\n",
    "# policy = Policy()\n",
    "# x = Variable(torch.Tensor([1, 2, 3, 4]))\n",
    "\n",
    "# probs = policy.forward(x)\n",
    "# m = Categorical(probs)\n",
    "# action = m.sample()\n",
    "# print(probs, m, action, m.log_prob(action))\n",
    "\n",
    "# action_pr = probs.detach().numpy()\n",
    "# action = np.random.choice(len(action_pr), p=action_pr)\n",
    "# print(action_pr, action, np.log(action_pr[action]), np.log(1-action_pr[action]))\n",
    "\n",
    "# # for param in policy.parameters():\n",
    "# #     print (param)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
