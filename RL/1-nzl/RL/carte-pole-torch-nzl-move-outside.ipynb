{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.seed(1); torch.manual_seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(torch.nn.Module):\n",
    "    def __init__(self, n_inputs=4, n_hidden=10, n_outputs=2, learning_rate=0.01, gamma=0.99):\n",
    "        super(Policy, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(n_inputs, n_hidden) \n",
    "        self.layer2 = torch.nn.Linear(n_hidden, n_outputs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.episode_data = []\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=1e-2)\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #l1z = self.layer1(x)\n",
    "        l1a = F.relu(self.layer1(x))\n",
    "        \n",
    "        l2z = self.layer2(l1a)\n",
    "        out = F.softmax(l2z, dim=0)\n",
    "        return out    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(policy):\n",
    "    R = 0\n",
    "    rewards = []\n",
    "    probs = np.array(policy.episode_data)[:,0]\n",
    "    ep_rewards = np.array(policy.episode_data)[:,1]\n",
    "    for r in reversed(ep_rewards):\n",
    "        R = r + policy.gamma*R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + policy.eps)\n",
    "\n",
    "    loss = np.dot(-probs, rewards)\n",
    "    policy.zero_grad()\n",
    "    #policy.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #policy.optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        for param in policy.parameters():\n",
    "            param.data -= policy.learning_rate*param.grad\n",
    "#         print(self.episode_data) # debug\n",
    "#         print(probs)\n",
    "#         print(ep_rewards)\n",
    "#         print(rewards)\n",
    "    policy.loss = loss\n",
    "    policy.episode_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(env, policy, episode_num=1000, render_num=100, log_interval=100):\n",
    "    for ep_idx in range(episode_num):\n",
    "        state = env.reset()\n",
    "        state = Variable(torch.Tensor(state))\n",
    "        for step_idx in range(1000):\n",
    "            action_probs = policy.forward(state)\n",
    "            m = Categorical(action_probs)\n",
    "            action = m.sample()\n",
    "#             action_probs = action_probs.detach().numpy()\n",
    "#             action = np.random.choice(len(action_probs), p=action_probs)\n",
    "#             action_prob = action_probs[action]\n",
    "            next_s, reward, done, _ = env.step(action.item())\n",
    "#             if ep_idx > render_num:\n",
    "#                 env.render()\n",
    "            policy.episode_data.append((m.log_prob(action), reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = Variable(torch.Tensor(next_s))\n",
    "        \n",
    "        learn(policy)\n",
    "        #if ep_idx % log_interval == 0:\n",
    "        print('\\rEpisode {}\\tLast length: {:5d}'.format(ep_idx, step_idx+1), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# policy = Policy()\n",
    "# x = Variable(torch.Tensor([1, 2, 3, 4]))\n",
    "\n",
    "# probs = policy.forward(x)\n",
    "# m = Categorical(probs)\n",
    "# action = m.sample()\n",
    "# print(probs, m, action, m.log_prob(action))\n",
    "\n",
    "# action_pr = probs.detach().numpy()\n",
    "# action = np.random.choice(len(action_pr), p=action_pr)\n",
    "# print(action_pr, action, np.log(action_pr[action]), np.log(1-action_pr[action]))\n",
    "\n",
    "# # for param in policy.parameters():\n",
    "# #     print (param)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 299\tLast length:   144"
     ]
    }
   ],
   "source": [
    "main(env, policy, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3851, grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
